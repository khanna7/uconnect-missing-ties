
R version 3.2.1 Patched (2015-07-12 r68650) -- "World-Famous Astronaut"
Copyright (C) 2015 The R Foundation for Statistical Computing
Platform: x86_64-unknown-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

[Previously saved workspace restored]

> library(network)
network: Classes for Relational Data
Version 1.12.0 created on 2015-03-04.
copyright (c) 2005, Carter T. Butts, University of California-Irvine
                    Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Martina Morris, University of Washington
                    Skye Bender-deMoll, University of Washington
 For citation information, type citation("network").
 Type help("network-package") to get started.

> library(ergm)
Loading required package: statnet.common

ergm: version 3.4.0, created on 2015-06-16
Copyright (c) 2015, Mark S. Handcock, University of California -- Los Angeles
                    David R. Hunter, Penn State University
                    Carter T. Butts, University of California -- Irvine
                    Steven M. Goodreau, University of Washington
                    Pavel N. Krivitsky, University of Wollongong
                    Martina Morris, University of Washington
                    with contributions from
                    Li Wang
                    Kirk Li, University of Washington
Based on "statnet" project software (statnet.org).
For license and citation information see statnet.org/attribution
or type citation("ergm").

NOTE: If you use custom ERGM terms based on ‘ergm.userterms’ version
prior to 3.1, you will need to perform a one-time update of the package
boilerplate files (the files that you did not write or modify) from
‘ergm.userterms’ 3.1 or later. See help('eut-upgrade') for
instructions.

> 
> load("mynet.RData")
> 
> mynet
 Network attributes:
  vertices = 50 
  directed = FALSE 
  hyper = FALSE 
  loops = FALSE 
  multiple = FALSE 
  bipartite = FALSE 
  total edges= 44 
    missing edges= 0 
    non-missing edges= 44 

 Vertex attribute names: 
    vertex.names 

No edge attributes
> mynet.mat <- as.matrix(mynet)
> mynet[1:nrow(mynet.mat), 1:ncol(mynet.mat)] <- NA
> 
> 
> ##
> # Source: http://www.umbc.edu/hpcf/resources-tara/how-to-run-R.html
> # filename: snow-test.R
> # 
> # SNOW quick ref: http://www.sfu.ca/~sblay/R/snow.html
> #
> # Notes:
> #   - Library loading order matters
> #   - system.time([function]) is an easy way to test optimizations
> #   - parApply is snow parallel version of 'apply'
> #
> ##
> 
> ## library(Rmpi)
> ## library(snow)
> 
> ## # Initialize SNOW using MPI communication. The first line will get the number of
> ## # MPI processes the scheduler assigned to us. Everything else is standard SNOW
> ## np <- mpi.universe.size() - 1
> ## cluster <- makeMPIcluster(np)
> 
> ## # Print the hostname for each cluster member
> ## sayhello <- function() {
> ##   info <- Sys.info()[c("nodename", "machine")]
> ##   paste("Hello from", info[1], "with CPU type", info[2])
> ## }
> 
> ## names <- clusterCall(cluster, sayhello)
> ## print(unlist(names))
> 
> ## # Compute row sums in parallel using all processes, then a grand sum at the end
> ## # on the master process
> ## parallelSum <- function(m, n) {
> ##   A <- matrix(rnorm(m*n), nrow = m, ncol = n)
> ##   # Parallelize the summation
> ##   row.sums <- parApply(cluster, A, 1, sum)
> ##   print(sum(row.sums))
> ## }
> 
> ## # Run the operation over different size matricies
> ## system.time(parallelSum(5000, 5000))
> 
> ## # Always stop your cluster and exit MPI to ensure resources are properly freed
> ## stopCluster(cluster)
> ## mpi.exit()
> 
> proc.time()
   user  system elapsed 
  4.791   0.099   4.869 
